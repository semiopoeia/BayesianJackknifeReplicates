This is intended to combine the benefits of bayesian inference for complex data structures with jackknife replicate weighting for complex survey designs. In this case, we've decided to include a state-level random effect which creates issues with insufficient memory if trying to use something like generalized linear mixed models from a standard frequentist-likelihood framework. Because parameters are naturally random in a bayesian context, i wanted to utilize this while also maintaining jackknife replicate weigthing. Though i tend to prefer bayesian inference, i see the benefit of frequentist thinking when it comes to adjusting a sample to the population, which is central to the logic of jackknifing and other re-sampling approaches. The core to this is to apply the weighting to the likelihood function in bayesian inference, i've chosen what i see as weakly informative priors, coding and conceptualization assistance came through some hodge-podge of Gemini, ChatGPT, and CoPilot. I've chose SAS because this is the preferred (and sometimes required) program for complex surveys from government agencies such as CDC (stata is often permitted, but i find it doesn't perform so well with computationally heavy stuff, while SAS sort of "burns right through" them). In the postestimation, there is flexibility as to whether we use the posterior distributions or lean into the jackknife inference. As this is a developing area of interest for me i would like to think through different ways that we might make inference by pulling across the posterior distributions and fully capitalize on the information provided for more bayesian (or blended) style inference. From the computational angle, even for the very simple model that i've fit (simple binomial logistic regression) as proof of concept/operational check, with 128 GB of RAM and as i9 CPU the run took nearly 3 hours with 80 replicate +1 full sample weighting, to get 1000 posterior draws per replication i used default 1000 MCMC burn-in, 10000 MCMC iterations, and thinning at 10. The diagnsotic plots for each replication looked good, going forward i'd be interested in parallelization to speed up run, looping over outcomes (though i might need to send this to a high-performance scientific research computer), as it stands the code should flexibly handle multiple predictors, however, i would have to think more closely and adapt for more intricate model specifications (e.g., non-additivity, non-linearity, model selection, etc.). I also look forward to keeping up with developments in empirical likelihood methodologies and perhaps one day having a better understanding and capacity to figure out how those might be incorporated to serve these kinds of purposes with greater flexibility and functionality.
